<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="一条想做废物的咸鱼">
<meta property="og:type" content="website">
<meta property="og:title" content="Mr.M">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Mr.M">
<meta property="og:description" content="一条想做废物的咸鱼">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Mr.M">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Mr.M</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Mr.M</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/20/Word2Vec/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mr.M">
      <meta itemprop="description" content="一条想做废物的咸鱼">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mr.M">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/20/Word2Vec/" class="post-title-link" itemprop="url">Word2Vec</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-06-20 15:46:07" itemprop="dateCreated datePublished" datetime="2021-06-20T15:46:07+08:00">2021-06-20</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%88%86%E7%B1%BB/" itemprop="url" rel="index"><span itemprop="name">分类</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/19/transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mr.M">
      <meta itemprop="description" content="一条想做废物的咸鱼">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mr.M">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/19/transformer/" class="post-title-link" itemprop="url">transformer</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-06-19 17:11:28" itemprop="dateCreated datePublished" datetime="2021-06-19T17:11:28+08:00">2021-06-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-06-20 15:34:41" itemprop="dateModified" datetime="2021-06-20T15:34:41+08:00">2021-06-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="transformer模型">Transformer模型</h4>
<ul>
<li>论文题目：Attention is all you need</li>
<li>链接：https://arxiv.org/abs/1706.03762</li>
<li>创新点：
<ul>
<li>通过self-attention，自己和自己做attention，使得每个词都有全局的语义信息（长距离依赖）</li>
<li>由于 Self-Attention 是每个词和所有词都要计算 Attention，所以不管他们中间有多长距离，最大的路径长度也都只是 1。可以捕获长距离依赖关系。</li>
<li>提出multi-head attention，可以看成attention的ensemble版本，不同head学习不同的子空间语义。</li>
</ul></li>
</ul>
<hr />
<h5 id="self-attention">Self-Attention</h5>
<p>​ 以一般的RNN的S2S为例子，一般的attention的Q来自Decoder（如下图中的大H），K和V来自Encoder（如下图中的小h）。self-attention就是attention的K、Q、V都来自encoder或者decoder，使得每个位置的表示都具有全局的语义信息，有利于建立长依赖关系。<img src="https://i.loli.net/2020/11/07/nBfi1yD4MZYc9gK.png" alt="image-20201107163443295" /></p>
<blockquote>
<p>self-attention的输入：序列词向量，假设为<span class="math inline">\(X\)</span>，<span class="math inline">\(X\)</span>经过三个线性变换得到Q，K，V，即<span class="math inline">\(X\times W_Q=Q,X\times W_K=K,X\times W_V=V\)</span>，其中<span class="math inline">\(W_Q,W_K,W_V\)</span>是不同的，通过训练得到。 <span class="math display">\[
Attention(Q,K,V)=Softmax(\frac{QK^T}{\sqrt{d}})V
\]</span></p>
<p>优点：</p>
<p>1.可以并行化</p>
<p>2.学习远程依赖</p>
<p>transformer中self-attention的维度dim<span class="math inline">\(=\frac{hidden}{num\_head\_number}\)</span></p>
</blockquote>
<h5 id="layer-normalization">Layer Normalization</h5>
<p>​ <strong>Batch Normalization</strong>是对一个每一个节点，针对一个batch，做一次normalization，即纵向的normalization: <img src="https://i.loli.net/2020/11/07/i3aM8ZlhTpuHDAP.png" alt="image-20201107163849892" /> layer normalization(LN)，是对一个样本，同一个层网络的所有神经元做normalization，不涉及到batch的概念，即横向normalization: <img src="https://i.loli.net/2020/11/07/o8HXFRDrdx4Gwcv.png" alt="image-20201107163933090" /> ​ BN适用于不同mini batch数据分布差异不大的情况，而且BN需要开辟变量存每个节点的均值和方差，空间消耗略大；而且 BN适用于有mini_batch的场景。 ​ LN只需要一个样本就可以做normalization，可以避免 BN 中受 mini-batch 数据分布影响的问题，也不需要开辟空间存每个节点的均值和方差。 ​ 但是，BN 的转换是针对单个神经元可训练的——不同神经元的输入经过再平移和再缩放后分布在不同的区间，而 LN 对于一整层的神经元训练得到同一个转换——所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小，scale不一样），那么 LN 的处理可能会降低模型的表达能力。</p>
<h4 id="postional-encoding">Postional Encoding</h4>
<blockquote>
<p>偶数列用sin函数激活</p>
<p>奇数列用cos函数激活</p>
</blockquote>
<h4 id="ffn">FFN</h4>
<blockquote>
<p>FFN包含两个线性变换，并且在第一个线性变换后加入ReLU激活函数 <span class="math display">\[
FFN (x)=max(0,xW_1+b_1)W_2+b_2
\]</span></p>
</blockquote>
<h4 id="encoder">Encoder</h4>
<blockquote>
<p>Encoder层：由6个相同的层堆叠而成，每个层包含两个子层，多头注意力层和FFN层。在子层之间加入残差连接和Layer Normalization。</p>
</blockquote>
<h4 id="decoder">Decoder</h4>
<blockquote>
<ul>
<li>Encoder与Decoder的不同</li>
<li>Decoder SubLayer-1 使用的是 “Masked” Multi-Headed Attention 机制，确保在生成时不引入当前词的下文信息，仅依赖于当前词之前的输出信息。这里的mask是一个下三角矩阵，对角线以及对角线左下都是1，其余都是0。</li>
<li>SubLayer-2 是一个 Encoder-Decoder Multi-head Attention。将<span class="math inline">\(h_{t}^{dec}\)</span>进行线性映射得到<span class="math inline">\(q_{t}^{dec}\)</span>,将<span class="math inline">\(q_{t}^{dec}\)</span>作为查询向量，通过键值对注意力机制来从输入<span class="math inline">\((K^{enc},V^{enc})\)</span>中选取有用的信息。</li>
</ul>
</blockquote>
<h4 id="qa">QA</h4>
<blockquote>
<p><strong>Ｑ：Transformer为何使用多头注意力机制？（为什么不使用一个头）</strong></p>
<p>Ａ：多头可以使参数矩阵形成多个子空间，矩阵整体的size不变，只是改变了每个head对应的维度大小，这样做使矩阵对多方面信息进行学习，但是计算量和单个head差不多。</p>
<p><strong>Ｑ：Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？</strong></p>
<p>Ａ：请求和键值初始为不同的权重是为了解决可能输入句长与输出句长不一致的问题。并且假如QK维度一致，如果不用Q，直接拿K和K点乘的话,即<span class="math inline">\(KK^T\)</span>，结果为一个对称矩阵，你会发现attention score 矩阵是一个对称矩阵。因为是同样一个矩阵，都投影到了同样一个空间，所以<strong>泛化能力很差</strong>。</p>
<p><strong>Ｑ：Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？</strong></p>
<p>Ａ：K和Q的点乘是为了得到一个attention score 矩阵，用来<strong>对V进行提纯</strong>。K和Q使用了不同的<span class="math inline">\(W_k, W_Q\)</span>来计算，可以理解为是在不同空间上的投影。正因为 有了这种不同空间的投影，增加了表达能力，这样计算得到的attention score矩阵的泛化能力更高。</p>
<p><strong>Ｑ：为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解</strong></p>
<p>Ａ：假设 Q 和 K 的均值为0，方差为1。它们的矩阵乘积<span class="math inline">\(q\cdot k=\sum_{i=1}^{d_k}q_ik_i\)</span>的均值为0，方差为<span class="math inline">\(d_k\)</span>，因此使用<span class="math inline">\(d_k\)</span>的平方根被用于缩放，因为，Q 和 K 的矩阵乘积的均值本应该为 0，方差本应该为1，这样可以获得更平缓的softmax。当维度很大时，点积结果会很大，会导致softmax的梯度很小。为了减轻这个影响，对点积进行缩放。<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/339723385/answer/782509914">知乎解答</a></p>
<p><strong>Ｑ：在计算attention score的时候如何对padding做mask操作？</strong></p>
<p>Ａ：对需要mask的位置设为负无穷，再对attention score进行相加</p>
<p><strong>Ｑ：为什么在进行多头注意力的时候需要对每个head进行降维？</strong></p>
<p>Ａ：将原有的高维空间转化为多个<strong>低维</strong>空间并再最后进行拼接，形成同样维度的输出，借此<strong>丰富特征信息</strong>，<strong>降低了计算量</strong></p>
<p><strong>Ｑ：为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？</strong></p>
<p>Ａ：embedding matrix的初始化方式是xavier init，这种方式的方差是1/embedding size，因此乘以embedding size的开方使得embedding matrix的方差是1，在这个scale下可能更有利于embedding matrix的收敛。</p>
<p><strong>Ｑ：简单介绍一下Transformer的位置编码？有什么意义和优缺点？</strong></p>
<p>Ａ：因为self-attention是位置无关的，无论句子的顺序是什么样的，通过self-attention计算的token的hidden embedding都是一样的，这显然不符合人类的思维。因此要有一个办法能够在模型中表达出一个token的位置信息，transformer使用了固定的positional encoding来表示token在句子中的绝对位置信息。</p>
<p><strong>Q：你还了解哪些关于位置编码的技术，各自的优缺点是什么？</strong></p>
<p>A：相对位置编码（RPE）1.在计算attention score和weighted value时各加入一个可训练的表示相对位置的参数。2.在生成多头注意力时，把对key来说将绝对位置转换为相对query的位置3.复数域函数，已知一个词在某个位置的词向量表示，可以计算出它在任何位置的词向量表示。前两个方法是词向量+位置编码，属于亡羊补牢，复数域是生成词向量的时候即生成对应的位置信息。</p>
<p><strong>11.简单讲一下Transformer中的残差结构以及意义。</strong></p>
<p>答：encoder和decoder的self-attention层和ffn层都有残差连接。反向传播的时候不会造成梯度消失。</p>
<p><strong>Q：为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？</strong></p>
<p>A：多头注意力层和激活函数层之间。CV使用BN是认为channel维度的信息对cv方面有重要意义，如果对channel维度也归一化会造成不同通道信息一定的损失。而同理nlp领域认为句子长度不一致，并且各个batch的信息没什么关系，因此只考虑句子内信息的归一化，也就是LN。</p>
<p><strong>Q：简答讲一下BatchNorm技术，以及它的优缺点。</strong></p>
<p>A：批归一化是对每一批的数据在进入激活函数前进行归一化，可以<strong>提高收敛速度</strong>，<strong>防止过拟合</strong>，<strong>防止梯度消失</strong>，增加网络对数据的敏感度。</p>
<p><strong>Q：简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？</strong></p>
<p>A：输入嵌入-加上位置编码-多个编码器层（每个编码器层包含全连接层，多头注意力层和点式前馈网络层（包含激活函数层））-多个解码器层（每个编码器层包含全连接层，多头注意力层和点式前馈网络层）-全连接层，使用了relu激活函数</p>
<p><strong>Q：Encoder端和Decoder端是如何进行交互的？</strong></p>
<p>A：通过转置encoder_ouput的seq_len维与depth维，进行矩阵两次乘法，即q<em>kT</em>v输出即可得到target_len维度的输出</p>
<p><strong>Q：Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？</strong></p>
<p>A：Decoder有<strong>两层</strong>mha，encoder有<strong>一层</strong>mha，Decoder的第二层mha是为了转化输入与输出句长，Decoder的请求q与键k和数值v的倒数第二个维度可以不一样，但是encoder的qkv维度一样。</p>
<p><strong>Q：Transformer的并行化提现在哪个地方？</strong></p>
<p>A：Transformer的并行化主要体现在self-attention模块，在Encoder端Transformer可以并行处理整个序列，并得到整个输入序列经过Encoder端的输出，但是rnn只能从前到后的执行</p>
<p><strong>Q：Transformer的并行化提现在哪个地方？</strong></p>
<p>A：Transformer的并行化主要体现在self-attention模块，在Encoder端Transformer可以并行处理整个序列，并得到整个输入序列经过Encoder端的输出，但是rnn只能从前到后的执行</p>
<p><strong>Q：简单描述一下wordpiece model 和 byte pair encoding，有实际应用过吗？</strong></p>
<p>A：“传统词表示方法无法很好的处理未知或罕见的词汇（<strong>OOV问题</strong>） 传统词tokenization方法不利于模型学习词缀之间的关系”</p>
<p>BPE（字节对编码）或二元编码是一种简单的数据压缩形式，其中最常见的一对连续字节数据被替换为该数据中不存在的字节。后期使用时需要一个替换表来重建原始数据。</p>
<p>优点：可以有效地平衡词汇表大小和步数（编码句子所需的token次数）。 缺点：基于贪婪和确定的符号替换，不能提供带概率的多个分片结果。</p>
<p><strong>Q.Transformer训练的时候学习率是如何设定的？Dropout是如何设定的，位置在哪里？Dropout 在测试的需要有什么需要注意的吗？</strong></p>
<p>A: LN是为了解决梯度消失的问题，dropout是为了解决过拟合的问题。在embedding后面加LN有利于embedding matrix的收敛。</p>
<p><strong>Q：bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？</strong></p>
<p>A：BERT和transformer的<strong>目标不一致</strong>，bert是语言的预训练模型，需要充分考虑上下文的关系，而transformer主要考虑句子中第i个元素与前i-1个元素的关系。</p>
<p><strong>Ｑ：BERT 模型为什么要用 mask？它是如何做 mask 的？其 mask 相对于 CBOW 有什么异同点？</strong></p>
<p>Ａ： <strong>BERT 模型为什么要用 mask?</strong> BERT 通过在输入 X 中随机 Mask 掉一部分单词，然后预训练过程的主要任务之一是根据上下文单词来预测这些被 Mask 掉的单词。其实这个就是典型的 Denosing Autoencoder 的思路，那些被 Mask 掉的单词就是<strong>在输入侧加入的所谓噪音。</strong>类似 BERT 这种预训练模式，被称为 DAE LM。因此总结来说 BERT 模型 [Mask] 标记就是引入噪音的手段。</p>
<p>​ <strong>它是如何做 mask 的？</strong>给定一个句子，会随机 Mask 15%的词，然后让 BERT 来预测这些 Mask 的词，如同上述 10.1 所述，在输入侧引入[Mask]标记，会导致预训练阶段和 Fine-tuning 阶段不一致的问题，因此在论文中为了缓解这一问题，采取了如下措施：</p>
<p>​ 如果某个 Token 在被选中的 15%个 Token 里，则按照下面的方式随机的执行：</p>
<p>​ 80%的概率替换成[MASK]，比如 my dog is hairy → my dog is [MASK] 10%的概率替换成随机的一个词，比如 my dog is hairy → my dog is apple 10%的概率替换成它本身，比如 my dog is hairy → my dog is hairy 这样做的好处是，BERT 并不知道[MASK]替换的是这 15%个 Token 中的哪一个词(「注意：这里意思是输入的时候不知道[MASK]替换的是哪一个词，但是输出还是知道要预测哪个词的」)，而且任何一个词都有可能是被替换掉的，比如它看到的 apple 可能是被替换的词。这样强迫模型在编码当前时刻的时候不能太依赖于当前的词，而要考虑它的上下文，甚至对其上下文进行”纠错”。</p>
<p>​ 其 mask 相对于 CBOW 有什么异同点？</p>
<p>​ 「相同点」：CBOW 的核心思想是：给定上下文，根据它的上文 Context-Before 和下文 Context-after 去预测 input word。而 BERT 本质上也是这么做的，但是 BERT 的做法是给定一个句子，会随机 Mask 15%的词，然后让 BERT 来预测这些 Mask 的词。</p>
<p>​ 「不同点」：首先，在 CBOW 中，每个单词都会成为 input word，而 BERT 不是这么做的，原因是这样做的话，训练数据就太大了，而且训练时间也会非常长。</p>
<p>​ 其次，对于输入数据部分，CBOW 中的输入数据只有待预测单词的上下文，而 BERT 的输入是带有[MASK] token 的“完整”句子，也就是说 BERT 在输入端将待预测的 input word 用[MASK] token 代替了。</p>
<p>​ 另外，通过 CBOW 模型训练后，每个单词的 word embedding 是唯一的，因此并不能很好的处理一词多义的问题，而 BERT 模型得到的 word embedding(token embedding)融合了上下文的信息，就算是同一个单词，在不同的上下文环境下，得到的 word embedding 是不一样的。</p>
<p><strong>Q：BERT的优缺点</strong></p>
<p>A：优点： 并行，解决长时依赖，双向特征表示，特征提取能力强，有效捕获上下文的全局信息，缓解梯度消失的问题等，BERT擅长解决的NLU任务。</p>
<p>缺点： 生成任务表现不佳：预训练过程和生成过程的不一致，导致在生成任务上效果不佳； 采取独立性假设：没有考虑预测[MASK]之间的相关性，是对语言模型联合概率的有偏估计（不是密度估计）； 输入噪声[MASK]，造成预训练-精调两阶段之间的差异； 无法文档级别的NLP任务，只适合于句子和段落级别的任务；</p>
<p><strong>Q：Transformer 在哪里做了权重共享，为什么可以做权重共享？</strong></p>
<p>A：</p>
<ol type="1">
<li>Encoder 和 Decoder 间的 Embedding 层权重共享；</li>
<li>Decoder 中 Embedding 层和 Full Connect（FC）层权重共享。</li>
</ol>
<p>对于第一点，《Attention is all you need》中 Transformer 被应用在机器翻译任务中，源语言和目标语言是不一样的，但它们可以共用一张大词表，对于两种语言中共同出现的词（比如：数字，标点等等）可以得到更好的表示，而且对于 Encoder 和 Decoder，嵌入时都只有对应语言的 embedding 会被激活，因此是可以共用一张词表做权重共享的。</p>
<p>论文中，Transformer 词表用了 bpe 来处理，所以最小的单元是 subword。英语和德语同属日耳曼语族，有很多相同的 subword，可以共享类似的语义。而像中英这样相差较大的语系，语义共享作用可能不会很大 [1]。</p>
<p>但是，共用词表会使得词表数量增大，增加 softmax 的计算时间，因此实际使用中是否共享可能要根据情况权衡。</p>
<p>对于第二点，Embedding 层可以说是通过 onehot 去取到对应的 embedding 向量，FC 层可以说是相反的，通过向量（定义为 w）去得到它可能是某个词的 softmax 概率，取概率最大（贪婪情况下）的作为预测值。</p>
<p>那哪一个会是概率最大的呢？在 FC 层的每一行量级相同的前提下，理论上和 w 相同的那一行对应的点积和 softmax 概率会是最大的（可类比本文问题 1）。</p>
<p>因此，Embedding 层和 FC 层权重共享，Embedding 层中和向量 w 最接近的那一行对应的词，会获得更大的预测概率。实际上，Embedding 层和 FC 层有点像互为逆过程。</p>
<p>通过这样的权重共享可以减少参数的数量，加快收敛。</p>
<p><strong>Q：如何减少训练好的神经网络模型的推理时间？</strong></p>
<p>A：1. 剪枝 2.知识蒸馏 3.16位的量化</p>
</blockquote>
<h4 id="refenrence">Refenrence</h4>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/lty_sky/article/details/105321180?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3.not_use_machine_learn_pai&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3.not_use_machine_learn_pai">代码和原理</a></p>
<p><a target="_blank" rel="noopener" href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">Harvard版本代码和原理</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/55b4de3de410">transformer、BERT常见面试题</a></p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Mr.M</p>
  <div class="site-description" itemprop="description">一条想做废物的咸鱼</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mr.M</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
