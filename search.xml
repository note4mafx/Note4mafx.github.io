<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>transformer</title>
    <url>/2021/06/19/transformer/</url>
    <content><![CDATA[<h4 id="transformer模型">Transformer模型</h4>
<ul>
<li>论文题目：Attention is all you need</li>
<li>链接：https://arxiv.org/abs/1706.03762</li>
<li>创新点：
<ul>
<li>通过self-attention，自己和自己做attention，使得每个词都有全局的语义信息（长距离依赖）</li>
<li>由于 Self-Attention 是每个词和所有词都要计算 Attention，所以不管他们中间有多长距离，最大的路径长度也都只是 1。可以捕获长距离依赖关系。</li>
<li>提出multi-head attention，可以看成attention的ensemble版本，不同head学习不同的子空间语义。</li>
</ul></li>
</ul>
<hr />
<h5 id="self-attention">Self-Attention</h5>
<p>​ 以一般的RNN的S2S为例子，一般的attention的Q来自Decoder（如下图中的大H），K和V来自Encoder（如下图中的小h）。self-attention就是attention的K、Q、V都来自encoder或者decoder，使得每个位置的表示都具有全局的语义信息，有利于建立长依赖关系。<img src="https://i.loli.net/2020/11/07/nBfi1yD4MZYc9gK.png" alt="image-20201107163443295" /></p>
<blockquote>
<p>self-attention的输入：序列词向量，假设为<span class="math inline">\(X\)</span>，<span class="math inline">\(X\)</span>经过三个线性变换得到Q，K，V，即<span class="math inline">\(X\times W_Q=Q,X\times W_K=K,X\times W_V=V\)</span>，其中<span class="math inline">\(W_Q,W_K,W_V\)</span>是不同的，通过训练得到。 <span class="math display">\[
Attention(Q,K,V)=Softmax(\frac{QK^T}{\sqrt{d}})V
\]</span></p>
<p>优点：</p>
<p>1.可以并行化</p>
<p>2.学习远程依赖</p>
<p>transformer中self-attention的维度dim<span class="math inline">\(=\frac{hidden}{num\_head\_number}\)</span></p>
</blockquote>
<h5 id="layer-normalization">Layer Normalization</h5>
<p>​ <strong>Batch Normalization</strong>是对一个每一个节点，针对一个batch，做一次normalization，即纵向的normalization: <img src="https://i.loli.net/2020/11/07/i3aM8ZlhTpuHDAP.png" alt="image-20201107163849892" /> layer normalization(LN)，是对一个样本，同一个层网络的所有神经元做normalization，不涉及到batch的概念，即横向normalization: <img src="https://i.loli.net/2020/11/07/o8HXFRDrdx4Gwcv.png" alt="image-20201107163933090" /> ​ BN适用于不同mini batch数据分布差异不大的情况，而且BN需要开辟变量存每个节点的均值和方差，空间消耗略大；而且 BN适用于有mini_batch的场景。 ​ LN只需要一个样本就可以做normalization，可以避免 BN 中受 mini-batch 数据分布影响的问题，也不需要开辟空间存每个节点的均值和方差。 ​ 但是，BN 的转换是针对单个神经元可训练的——不同神经元的输入经过再平移和再缩放后分布在不同的区间，而 LN 对于一整层的神经元训练得到同一个转换——所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小，scale不一样），那么 LN 的处理可能会降低模型的表达能力。</p>
<h4 id="postional-encoding">Postional Encoding</h4>
<blockquote>
<p>偶数列用sin函数激活</p>
<p>奇数列用cos函数激活</p>
</blockquote>
<h4 id="ffn">FFN</h4>
<blockquote>
<p>FFN包含两个线性变换，并且在第一个线性变换后加入ReLU激活函数 <span class="math display">\[
FFN (x)=max(0,xW_1+b_1)W_2+b_2
\]</span></p>
</blockquote>
<h4 id="encoder">Encoder</h4>
<blockquote>
<p>Encoder层：由6个相同的层堆叠而成，每个层包含两个子层，多头注意力层和FFN层。在子层之间加入残差连接和Layer Normalization。</p>
</blockquote>
<h4 id="decoder">Decoder</h4>
<blockquote>
<ul>
<li>Encoder与Decoder的不同</li>
<li>Decoder SubLayer-1 使用的是 “Masked” Multi-Headed Attention 机制，确保在生成时不引入当前词的下文信息，仅依赖于当前词之前的输出信息。这里的mask是一个下三角矩阵，对角线以及对角线左下都是1，其余都是0。</li>
<li>SubLayer-2 是一个 Encoder-Decoder Multi-head Attention。将<span class="math inline">\(h_{t}^{dec}\)</span>进行线性映射得到<span class="math inline">\(q_{t}^{dec}\)</span>,将<span class="math inline">\(q_{t}^{dec}\)</span>作为查询向量，通过键值对注意力机制来从输入<span class="math inline">\((K^{enc},V^{enc})\)</span>中选取有用的信息。</li>
</ul>
</blockquote>
<h4 id="qa">QA</h4>
<blockquote>
<p><strong>Ｑ：Transformer为何使用多头注意力机制？（为什么不使用一个头）</strong></p>
<p>Ａ：多头可以使参数矩阵形成多个子空间，矩阵整体的size不变，只是改变了每个head对应的维度大小，这样做使矩阵对多方面信息进行学习，但是计算量和单个head差不多。</p>
<p><strong>Ｑ：Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？</strong></p>
<p>Ａ：请求和键值初始为不同的权重是为了解决可能输入句长与输出句长不一致的问题。并且假如QK维度一致，如果不用Q，直接拿K和K点乘的话,即<span class="math inline">\(KK^T\)</span>，结果为一个对称矩阵，你会发现attention score 矩阵是一个对称矩阵。因为是同样一个矩阵，都投影到了同样一个空间，所以<strong>泛化能力很差</strong>。</p>
<p><strong>Ｑ：Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？</strong></p>
<p>Ａ：K和Q的点乘是为了得到一个attention score 矩阵，用来<strong>对V进行提纯</strong>。K和Q使用了不同的<span class="math inline">\(W_k, W_Q\)</span>来计算，可以理解为是在不同空间上的投影。正因为 有了这种不同空间的投影，增加了表达能力，这样计算得到的attention score矩阵的泛化能力更高。</p>
<p><strong>Ｑ：为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解</strong></p>
<p>Ａ：假设 Q 和 K 的均值为0，方差为1。它们的矩阵乘积<span class="math inline">\(q\cdot k=\sum_{i=1}^{d_k}q_ik_i\)</span>的均值为0，方差为<span class="math inline">\(d_k\)</span>，因此使用<span class="math inline">\(d_k\)</span>的平方根被用于缩放，因为，Q 和 K 的矩阵乘积的均值本应该为 0，方差本应该为1，这样可以获得更平缓的softmax。当维度很大时，点积结果会很大，会导致softmax的梯度很小。为了减轻这个影响，对点积进行缩放。<a href="https://www.zhihu.com/question/339723385/answer/782509914">知乎解答</a></p>
<p><strong>Ｑ：在计算attention score的时候如何对padding做mask操作？</strong></p>
<p>Ａ：对需要mask的位置设为负无穷，再对attention score进行相加</p>
<p><strong>Ｑ：为什么在进行多头注意力的时候需要对每个head进行降维？</strong></p>
<p>Ａ：将原有的高维空间转化为多个<strong>低维</strong>空间并再最后进行拼接，形成同样维度的输出，借此<strong>丰富特征信息</strong>，<strong>降低了计算量</strong></p>
<p><strong>Ｑ：为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？</strong></p>
<p>Ａ：embedding matrix的初始化方式是xavier init，这种方式的方差是1/embedding size，因此乘以embedding size的开方使得embedding matrix的方差是1，在这个scale下可能更有利于embedding matrix的收敛。</p>
<p><strong>Ｑ：简单介绍一下Transformer的位置编码？有什么意义和优缺点？</strong></p>
<p>Ａ：因为self-attention是位置无关的，无论句子的顺序是什么样的，通过self-attention计算的token的hidden embedding都是一样的，这显然不符合人类的思维。因此要有一个办法能够在模型中表达出一个token的位置信息，transformer使用了固定的positional encoding来表示token在句子中的绝对位置信息。</p>
<p><strong>Q：你还了解哪些关于位置编码的技术，各自的优缺点是什么？</strong></p>
<p>A：相对位置编码（RPE）1.在计算attention score和weighted value时各加入一个可训练的表示相对位置的参数。2.在生成多头注意力时，把对key来说将绝对位置转换为相对query的位置3.复数域函数，已知一个词在某个位置的词向量表示，可以计算出它在任何位置的词向量表示。前两个方法是词向量+位置编码，属于亡羊补牢，复数域是生成词向量的时候即生成对应的位置信息。</p>
<p><strong>11.简单讲一下Transformer中的残差结构以及意义。</strong></p>
<p>答：encoder和decoder的self-attention层和ffn层都有残差连接。反向传播的时候不会造成梯度消失。</p>
<p><strong>Q：为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？</strong></p>
<p>A：多头注意力层和激活函数层之间。CV使用BN是认为channel维度的信息对cv方面有重要意义，如果对channel维度也归一化会造成不同通道信息一定的损失。而同理nlp领域认为句子长度不一致，并且各个batch的信息没什么关系，因此只考虑句子内信息的归一化，也就是LN。</p>
<p><strong>Q：简答讲一下BatchNorm技术，以及它的优缺点。</strong></p>
<p>A：批归一化是对每一批的数据在进入激活函数前进行归一化，可以<strong>提高收敛速度</strong>，<strong>防止过拟合</strong>，<strong>防止梯度消失</strong>，增加网络对数据的敏感度。</p>
<p><strong>Q：简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？</strong></p>
<p>A：输入嵌入-加上位置编码-多个编码器层（每个编码器层包含全连接层，多头注意力层和点式前馈网络层（包含激活函数层））-多个解码器层（每个编码器层包含全连接层，多头注意力层和点式前馈网络层）-全连接层，使用了relu激活函数</p>
<p><strong>Q：Encoder端和Decoder端是如何进行交互的？</strong></p>
<p>A：通过转置encoder_ouput的seq_len维与depth维，进行矩阵两次乘法，即q<em>kT</em>v输出即可得到target_len维度的输出</p>
<p><strong>Q：Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？</strong></p>
<p>A：Decoder有<strong>两层</strong>mha，encoder有<strong>一层</strong>mha，Decoder的第二层mha是为了转化输入与输出句长，Decoder的请求q与键k和数值v的倒数第二个维度可以不一样，但是encoder的qkv维度一样。</p>
<p><strong>Q：Transformer的并行化提现在哪个地方？</strong></p>
<p>A：Transformer的并行化主要体现在self-attention模块，在Encoder端Transformer可以并行处理整个序列，并得到整个输入序列经过Encoder端的输出，但是rnn只能从前到后的执行</p>
<p><strong>Q：Transformer的并行化提现在哪个地方？</strong></p>
<p>A：Transformer的并行化主要体现在self-attention模块，在Encoder端Transformer可以并行处理整个序列，并得到整个输入序列经过Encoder端的输出，但是rnn只能从前到后的执行</p>
<p><strong>Q：简单描述一下wordpiece model 和 byte pair encoding，有实际应用过吗？</strong></p>
<p>A：“传统词表示方法无法很好的处理未知或罕见的词汇（<strong>OOV问题</strong>） 传统词tokenization方法不利于模型学习词缀之间的关系”</p>
<p>BPE（字节对编码）或二元编码是一种简单的数据压缩形式，其中最常见的一对连续字节数据被替换为该数据中不存在的字节。后期使用时需要一个替换表来重建原始数据。</p>
<p>优点：可以有效地平衡词汇表大小和步数（编码句子所需的token次数）。 缺点：基于贪婪和确定的符号替换，不能提供带概率的多个分片结果。</p>
<p><strong>Q.Transformer训练的时候学习率是如何设定的？Dropout是如何设定的，位置在哪里？Dropout 在测试的需要有什么需要注意的吗？</strong></p>
<p>A: LN是为了解决梯度消失的问题，dropout是为了解决过拟合的问题。在embedding后面加LN有利于embedding matrix的收敛。</p>
<p><strong>Q：bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？</strong></p>
<p>A：BERT和transformer的<strong>目标不一致</strong>，bert是语言的预训练模型，需要充分考虑上下文的关系，而transformer主要考虑句子中第i个元素与前i-1个元素的关系。</p>
<p><strong>Ｑ：BERT 模型为什么要用 mask？它是如何做 mask 的？其 mask 相对于 CBOW 有什么异同点？</strong></p>
<p>Ａ： <strong>BERT 模型为什么要用 mask?</strong> BERT 通过在输入 X 中随机 Mask 掉一部分单词，然后预训练过程的主要任务之一是根据上下文单词来预测这些被 Mask 掉的单词。其实这个就是典型的 Denosing Autoencoder 的思路，那些被 Mask 掉的单词就是<strong>在输入侧加入的所谓噪音。</strong>类似 BERT 这种预训练模式，被称为 DAE LM。因此总结来说 BERT 模型 [Mask] 标记就是引入噪音的手段。</p>
<p>​ <strong>它是如何做 mask 的？</strong>给定一个句子，会随机 Mask 15%的词，然后让 BERT 来预测这些 Mask 的词，如同上述 10.1 所述，在输入侧引入[Mask]标记，会导致预训练阶段和 Fine-tuning 阶段不一致的问题，因此在论文中为了缓解这一问题，采取了如下措施：</p>
<p>​ 如果某个 Token 在被选中的 15%个 Token 里，则按照下面的方式随机的执行：</p>
<p>​ 80%的概率替换成[MASK]，比如 my dog is hairy → my dog is [MASK] ​ 10%的概率替换成随机的一个词，比如 my dog is hairy → my dog is apple ​ 10%的概率替换成它本身，比如 my dog is hairy → my dog is hairy ​ 这样做的好处是，BERT 并不知道[MASK]替换的是这 15%个 Token 中的哪一个词(「注意：这里意思是输入的时候不知道[MASK]替换的是哪一个词，但是输出还是知道要预测哪个词的」)，而且任何一个词都有可能是被替换掉的，比如它看到的 apple 可能是被替换的词。这样强迫模型在编码当前时刻的时候不能太依赖于当前的词，而要考虑它的上下文，甚至对其上下文进行”纠错”。</p>
<p>​ 其 mask 相对于 CBOW 有什么异同点？</p>
<p>​ 「相同点」：CBOW 的核心思想是：给定上下文，根据它的上文 Context-Before 和下文 Context-after 去预测 input word。而 BERT 本质上也是这么做的，但是 BERT 的做法是给定一个句子，会随机 Mask 15%的词，然后让 BERT 来预测这些 Mask 的词。</p>
<p>​ 「不同点」：首先，在 CBOW 中，每个单词都会成为 input word，而 BERT 不是这么做的，原因是这样做的话，训练数据就太大了，而且训练时间也会非常长。</p>
<p>​ 其次，对于输入数据部分，CBOW 中的输入数据只有待预测单词的上下文，而 BERT 的输入是带有[MASK] token 的“完整”句子，也就是说 BERT 在输入端将待预测的 input word 用[MASK] token 代替了。</p>
<p>​ 另外，通过 CBOW 模型训练后，每个单词的 word embedding 是唯一的，因此并不能很好的处理一词多义的问题，而 BERT 模型得到的 word embedding(token embedding)融合了上下文的信息，就算是同一个单词，在不同的上下文环境下，得到的 word embedding 是不一样的。</p>
<p><strong>Q：BERT的优缺点</strong></p>
<p>A：优点： 并行，解决长时依赖，双向特征表示，特征提取能力强，有效捕获上下文的全局信息，缓解梯度消失的问题等，BERT擅长解决的NLU任务。</p>
<p>缺点： 生成任务表现不佳：预训练过程和生成过程的不一致，导致在生成任务上效果不佳； 采取独立性假设：没有考虑预测[MASK]之间的相关性，是对语言模型联合概率的有偏估计（不是密度估计）； 输入噪声[MASK]，造成预训练-精调两阶段之间的差异； 无法文档级别的NLP任务，只适合于句子和段落级别的任务；</p>
<p><strong>Q：Transformer 在哪里做了权重共享，为什么可以做权重共享？</strong></p>
<p>A：</p>
<ol type="1">
<li>Encoder 和 Decoder 间的 Embedding 层权重共享；</li>
<li>Decoder 中 Embedding 层和 Full Connect（FC）层权重共享。</li>
</ol>
<p>对于第一点，《Attention is all you need》中 Transformer 被应用在机器翻译任务中，源语言和目标语言是不一样的，但它们可以共用一张大词表，对于两种语言中共同出现的词（比如：数字，标点等等）可以得到更好的表示，而且对于 Encoder 和 Decoder，嵌入时都只有对应语言的 embedding 会被激活，因此是可以共用一张词表做权重共享的。</p>
<p>论文中，Transformer 词表用了 bpe 来处理，所以最小的单元是 subword。英语和德语同属日耳曼语族，有很多相同的 subword，可以共享类似的语义。而像中英这样相差较大的语系，语义共享作用可能不会很大 [1]。</p>
<p>但是，共用词表会使得词表数量增大，增加 softmax 的计算时间，因此实际使用中是否共享可能要根据情况权衡。</p>
<p>对于第二点，Embedding 层可以说是通过 onehot 去取到对应的 embedding 向量，FC 层可以说是相反的，通过向量（定义为 w）去得到它可能是某个词的 softmax 概率，取概率最大（贪婪情况下）的作为预测值。</p>
<p>那哪一个会是概率最大的呢？在 FC 层的每一行量级相同的前提下，理论上和 w 相同的那一行对应的点积和 softmax 概率会是最大的（可类比本文问题 1）。</p>
<p>因此，Embedding 层和 FC 层权重共享，Embedding 层中和向量 w 最接近的那一行对应的词，会获得更大的预测概率。实际上，Embedding 层和 FC 层有点像互为逆过程。</p>
<p>通过这样的权重共享可以减少参数的数量，加快收敛。</p>
<p><strong>Q：如何减少训练好的神经网络模型的推理时间？</strong></p>
<p>A：1. 剪枝 2.知识蒸馏 3.16位的量化</p>
</blockquote>
<h4 id="refenrence">Refenrence</h4>
<blockquote>
<p><a href="https://blog.csdn.net/lty_sky/article/details/105321180?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3.not_use_machine_learn_pai&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3.not_use_machine_learn_pai">代码和原理</a></p>
<p><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">Harvard版本代码和原理</a></p>
<p><a href="https://www.jianshu.com/p/55b4de3de410">transformer、BERT常见面试题</a></p>
</blockquote>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>transformer</tag>
        <tag>model</tag>
      </tags>
  </entry>
  <entry>
    <title>Word2Vec</title>
    <url>/2021/06/20/Word2Vec/</url>
    <content><![CDATA[<h2 id="word2vec">Word2Vec</h2>
<blockquote>
<p>论文题目：《Efficient Estimation of Word Representations in Vector Space》（原始论文，主要从复杂度以及结果上分析，模型细节部分描述较少）</p>
<p>链接：https://arxiv.org/abs/1301.3781v3</p>
<p>论文题目：《Distributed Representations of Words and Phrases and their Compositionality》（层次softmax和负采样）</p>
<p>链接：https://arxiv.org/abs/1310.4546</p>
<p>论文题目：《word2vec parameter learning explained》（word2vec模型学习、公式等细节解释）</p>
<p>链接：https://arxiv.org/abs/1411.2738</p>
</blockquote>
<h3 id="word2vec的两种方法">Word2Vec的两种方法</h3>
<h4 id="cbow">CBOW</h4>
<p><img src="https://i.loli.net/2021/06/20/Wl7hKnpkFHR683O.png" alt="image-20210620163720557" style="zoom:50%;" /></p>
<p>CBOW模型是根据上下文单词预测中心词。隐藏层的输出为输入上下文单词的向量的平均值，与input layer-&gt;hidden layer的权值矩阵相乘。 <span class="math display">\[
\begin{align*}
h &amp;= \frac{1}{C}W^T(x_1+x_2+\cdots+x_C)\\
&amp;=\frac{1}{C}(v_{w1}+v_{w2}+\cdots+v_{wc})^T
\end{align*}
\]</span> 其中，<span class="math inline">\(C\)</span>为上下文的单词数，损失函数为给定上下文单词的权重，最大化下式 <span class="math display">\[
max P(W_O|W_{I,1},\cdots,W_{I,C})
\]</span></p>
<p>即最小化<span class="math inline">\(-logp(W_o|W_{I,1},\cdots,W_{I,C})\)</span></p>
<h4 id="skip-gram">Skip-gram</h4>
<p><img src="https://i.loli.net/2021/06/20/NfiqbX7mVlEeDWA.png" alt="image-20210620163631059" style="zoom:50%;" /></p>
<p>Skip-gram模型与CBOW模型相反，目标词在输入层，上下文单词则在输出层。即根据中心词预测上下文单词。</p>
<p>隐藏层为input layer-&gt;hidden layer中的一行，即 <span class="math display">\[
h = W^T_{(k,.)}:=v^T_{wI}
\]</span> <img src="https://i.loli.net/2021/06/20/ufoKkFTzVPWa4XU.png" alt="image-20210620171225676" style="zoom:50%;" /></p>
<p>输出层不是一个多项式分布，而输出<span class="math inline">\(C\)</span>个多项式分布，输出层共享参数。使用相同的hidden layer→output layer权重矩阵计算每个输出： <span class="math display">\[
p(W_{c,j}=W_{O,c}|W_I)=y_{c,j}=\frac{exp(u_{c,j})}{\sum_{j&#39;=1}^{V}exp(u_{j&#39;})}
\]</span></p>
<p><span class="math display">\[
u_{c,j}=u_j=v&#39;^{T}_{w_j}\cdot h, for \space c =1,2,\dots,C
\]</span></p>
<p>其中，<span class="math inline">\(W_{c,j}\)</span>是指第<span class="math inline">\(C\)</span>个分布中的第<span class="math inline">\(j\)</span>个单词，</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>词向量</tag>
        <tag>Word2Vec</tag>
      </tags>
  </entry>
  <entry>
    <title>模型压缩</title>
    <url>/2021/07/08/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/</url>
    <content><![CDATA[<h2 id="模型压缩">模型压缩</h2>
<ol type="1">
<li>网络剪枝(Pruning)：去掉模型中作用比较小的连接。</li>
<li>量化(Quantization)：降低大模型的精度，减小模型。</li>
<li>模型蒸馏(Distillation)：使用大模型学到的知识训练小模型，从而让小模型具有大模型的泛化能力。</li>
<li>参数共享：共享网络层的部分参数，降低模型参数数量。</li>
</ol>
<h3 id="网络剪枝">网络剪枝</h3>
<h3 id="量化">量化</h3>
<h4 id="tensorrt">TensorRT</h4>
<h5 id="tensorrt的优化方法">TensorRT的优化方法</h5>
<ul>
<li><p><strong>层间融合或张量融合（Layer &amp; Tensor Fusion）</strong></p>
<p>​ 在部署模型推理时，这每一层的运算操作都是由GPU完成的，但实际上是GPU通过启动不同的CUDA核心来完成计算的，CUDA核心计算张量的速度是很快的，但是往往大量的时间是浪费在CUDA核心的启动和对每一层输入/输出张量的读写操作上面，这造成了内存带宽的瓶颈和GPU资源的浪费。TensorRT通过对层间的横向或纵向合并（合并后的结构称为CBR，意指 convolution, bias, and ReLU layers are fused to form a single layer），使得层的数量大大减少。横向合并可以把卷积、偏置和激活层合并成一个CBR结构，只占用一个CUDA核心。纵向合并可以把结构相同，但是权值不同的层合并成一个更宽的层，也只占用一个CUDA核心。合并之后的计算图的层次更少了，占用的CUDA核心数也少了，因此整个模型结构会更小，更快，更高效。</p>
<figure>
<img src="https://i.loli.net/2021/07/06/fv5jLEcuDTCJVQR.png" alt="image-20210706105524675" /><figcaption>image-20210706105524675</figcaption>
</figure></li>
<li><p><strong>数据精度校准（Weight &amp;Activation Precision Calibration）</strong> 大部分深度学习框架在训练神经网络时网络中的张量（Tensor）都是32位浮点数的精度（Full 32-bit precision，FP32），一旦网络训练完成，在部署推理的过程中由于不需要反向传播，完全可以适当降低数据精度，比如降为FP16或INT8的精度。更低的数据精度将会使得内存占用和延迟更低，模型体积更小。</p></li>
</ul>
<h5 id="torch版bert转化">Torch版BERT转化</h5>
<ul>
<li><p>torch模型转ONNX</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> small_bert <span class="keyword">import</span> SmallBERT</span><br><span class="line"><span class="keyword">from</span> config.small_bert_config <span class="keyword">import</span> SmallBertConfig</span><br><span class="line"><span class="keyword">from</span> data_helper_bert <span class="keyword">import</span> encode_fn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">config = SmallBertConfig()</span><br><span class="line">model = SmallBERT(config)</span><br><span class="line"></span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;./model/small_bert.pth&#x27;</span>, map_location=<span class="string">&#x27;cpu&#x27;</span>))</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"><span class="comment"># for name, parameters in model.named_parameters():</span></span><br><span class="line"><span class="comment">#     print(name, &#x27;:&#x27;, parameters.size())</span></span><br><span class="line"></span><br><span class="line">input_ids, token_type_ids, attention_mask = encode_fn(<span class="string">&#x27;自然语言处理&#x27;</span>.split())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置seq_len维度是动态的</span></span><br><span class="line">dynamic_ax = &#123;<span class="string">&#x27;input_ids&#x27;</span>: [<span class="number">1</span>], <span class="string">&quot;token_type_ids&quot;</span>: [<span class="number">1</span>], <span class="string">&quot;attention_mask&quot;</span>: [<span class="number">1</span>]&#125;</span><br><span class="line"></span><br><span class="line">torch.onnx.export(model,</span><br><span class="line">                  (input_ids, token_type_ids, attention_mask),</span><br><span class="line">                  <span class="string">&#x27;./onnx/small_bert_dynamic.onnx&#x27;</span>,</span><br><span class="line">                  opset_version=<span class="number">10</span>,</span><br><span class="line">                  do_constant_folding=<span class="literal">True</span>,  <span class="comment"># 是否执行常量折叠优化</span></span><br><span class="line">                  verbose=<span class="literal">True</span>,</span><br><span class="line">                  input_names=[<span class="string">&quot;input_ids&quot;</span>, <span class="string">&quot;token_type_ids&quot;</span>, <span class="string">&quot;attention_mask&quot;</span>],<span class="comment">#输入名</span></span><br><span class="line">                  output_names=[<span class="string">&quot;fc&quot;</span>],  <span class="comment"># 输出名</span></span><br><span class="line">                  dynamic_axes=dynamic_ax <span class="comment"># 设置动态的输入输出</span></span><br><span class="line">                  )</span><br></pre></td></tr></table></figure></li>
<li><p>ONNX转TensorRT</p></li>
</ul>
<h3 id="知识蒸馏">知识蒸馏</h3>
<h4 id="蒸馏的做法">蒸馏的做法：</h4>
<ul>
<li>首先训练一个大的模型，称之为teacher模型。</li>
<li>利用teacher模型输出的概率分布训练小模型，称之为student模型。</li>
<li>训练student模型时，包含两种label，一种是soft-label，一种是hard-label。其中soft-label对应了teacher模型输出的概率分布，而hard-label则代表原始的one-hot label。</li>
<li>模型蒸馏训练得到的小模型，即student模型会学习到大模型的表现以及泛化能力。</li>
</ul>
<h4 id="相关细节">相关细节</h4>
<ul>
<li><p>带温度的softmax</p>
<ul>
<li>正常的softmax <span class="math inline">\(q_i=\frac{exp(z_i)}{\sum_jexp(z_j)}\)</span>，要是直接使用softmax层的输出值作为soft target, 会带来一个问题: 当softmax输出的概率分布熵相对较小时，负标签的值都很接近0，对损失函数的贡献非常小，小到可以忽略不计。因此&quot;温度&quot;这个变量就派上了用场。</li>
<li>带温度的softmax <span class="math inline">\(q_i=\frac{exp(\frac{z_i}{T})}{\sum_jexp(\frac{z_j}{T})}\)</span>，其中<span class="math inline">\(T\)</span>就是温度系数。当<span class="math inline">\(T=1\)</span>时，为正常的softmax。T越高，softmax的output probability distribution越趋于平滑，其分布的熵越大，负标签携带的信息会被相对地放大，模型训练将更加关注负标签。</li>
</ul></li>
<li><p><strong>温度的特点及选择</strong></p>
<ul>
<li>温度的特点
<ul>
<li>原始的softmax是温度系数<span class="math inline">\(T=1\)</span>时的特例，当<span class="math inline">\(T&lt;1\)</span>时，概率分布会比原始更陡峭，当<span class="math inline">\(T&gt;1\)</span>时，概率分布会比原始更平缓。温度越高，softmax上各个值的分布就越平均。比如，当<span class="math inline">\(T\rightarrow \infty\)</span>时，此时的softmax是平均分布。当<span class="math inline">\(T\rightarrow0\)</span>，此时的softmax在最大概率处的值<span class="math inline">\(\rightarrow1\)</span>，而其他值<span class="math inline">\(\rightarrow0\)</span>。</li>
</ul></li>
<li>温度的选择
<ul>
<li><strong>温度的高低代表着student网络在训练过程中对负标签的关注程度。</strong>温度较低时，对负标签的关注，尤其是那些显著低于平均值的负标签的关注较少；而温度较高时，负标签相关的值会相对增大，student网络会相对多地关注到负标签。</li>
<li>从有部分信息量的负标签中学习，此时温度要高一点。</li>
<li>防止受负标签中噪声的影响，此时温度要低一点。</li>
</ul></li>
</ul></li>
<li><p>知识蒸馏的过程</p>
<ul>
<li><p>流程示意 <img src="https://i.loli.net/2021/07/08/kUZYL9ysCIrmO1n.png" style="zoom:50%;" /></p></li>
<li><p>Loss计算 <span class="math display">\[
L = \alpha L_{soft}+\beta L_{hard} \\
L_{soft}=-\sum_j^Np_j^Tlog(q_j^T) \\
p_i^T=\frac{exp(v_i/T)}{\sum_k^Nexp(v_k/T)} \\
q_i^T=\frac{exp(z_i/T)}{\sum_k^Nexp(z_k/T)} \\
L_{hard}=-\sum_j^Nc_jlog(q_j^1) \\
q_i^1=\frac{exp(z_i)}{\sum_k^Nexp(z_k)}
\]</span> 其中，<span class="math inline">\(v_i\)</span>是teacher网络的logits，<span class="math inline">\(z_i\)</span>是student网络的logits，<span class="math inline">\(c_i\)</span>是在第<span class="math inline">\(i\)</span>类上的ground truth值，<span class="math inline">\(c_i\in\{0,1\}\)</span>，正标签为1，负标签为0。</p></li>
</ul></li>
<li><p><strong>KD的训练过程为什么更有效?</strong></p>
<blockquote>
<p>softmax层的输出，除了正例之外，负标签也带有大量的信息，比如某些负标签对应的概率远远大于其他负标签。而在传统的训练过程(hard target)中，所有负标签都被统一对待。也就是说，KD的训练方式使得每个样本给student网络带来的信息量大于传统的训练方式。</p>
</blockquote></li>
<li><p><strong>soft target之前乘上<span class="math inline">\(T^2\)</span>的系数？</strong></p>
<blockquote>
<p>由于<span class="math inline">\(\frac{\partial L_{soft}}{\partial z_i}\)</span>的梯度大约是<span class="math inline">\(\frac{\partial L_{hard}}{\partial z_i}\)</span>的<span class="math inline">\(\frac{1}{T^2}\)</span>，所以在同时使用soft target和hard target的时候，需要在soft target之前乘上<span class="math inline">\(T^2\)</span>的系数，这样才能保证soft target和hard target贡献的梯度量基本一致。</p>
</blockquote></li>
<li><p><strong>注意事项</strong></p>
<ul>
<li>在student网络训练完毕后，做inference时其softmax的温度T要恢复到1。</li>
</ul></li>
</ul>
<h4 id="reference">Reference</h4>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/102038521">知识蒸馏</a></p>
</blockquote>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>模型压缩</tag>
      </tags>
  </entry>
  <entry>
    <title>docker操作</title>
    <url>/2021/07/08/docker%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h3 id="创建新的容器">创建新的容器</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">-t # 创建一个虚拟终端</span><br><span class="line">-i # 将容器的标准输入保持打开</span><br><span class="line">--name # 指定名字</span><br><span class="line">-d # 后台运行</span><br><span class="line">-p # 宿主机端口:容器端口</span><br><span class="line">-v # 宿主机目录:容器指定目录(可以实现将宿主机目录挂载到容器中)</span><br><span class="line">nvidia-docker run -it -p 8011:8011 --name=bert_cla -v /data/person/:/data/ --restart=always bert_fact_cla:1.0</span><br></pre></td></tr></table></figure>
<h3 id="进入容器">进入容器</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker exec -ti 容器ID或者容器名称 bash</span><br></pre></td></tr></table></figure>
<h3 id="查看镜像">查看镜像</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker images</span><br></pre></td></tr></table></figure>
<h3 id="查看容器">查看容器</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker ps	#查看运行中的容器</span><br><span class="line">docker ps -a #查看所有的容器</span><br></pre></td></tr></table></figure>
<h3 id="从容器中创建一个新的镜像">从容器中创建一个新的镜像</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker commit 容器ID或者容器名称 镜像名:tag</span><br></pre></td></tr></table></figure>
<h3 id="停止重启启动镜像">停止、重启启动镜像</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker start id</span><br><span class="line">docker stop id</span><br><span class="line">docker restart id</span><br></pre></td></tr></table></figure>
<h3 id="删除镜像和容器">删除镜像和容器</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker rmi 镜像ID	#删除镜像</span><br><span class="line">docker rm 容器ID #删除容器</span><br><span class="line">docker rm -f 容器ID #删除容器</span><br></pre></td></tr></table></figure>
<h3 id="镜像导出">镜像导出</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker save -o bert_fact_cla.tar 镜像名:tag</span><br></pre></td></tr></table></figure>
<h3 id="镜像载入">镜像载入</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker load -i bert_fact_cla.tar</span><br></pre></td></tr></table></figure>
<h3 id="dockerfile的制作">dockerfile的制作</h3>
<p>​ 创建dockerfile所在的文件夹</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir tfgpu</span><br><span class="line">cd tfgpu</span><br><span class="line">vim DockerFile	#vim保存并退出 :wq</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">FROM nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04</span><br><span class="line">MAINTAINER Tyan &lt;tyan.lu.git@gmail.com&gt;</span><br><span class="line"><span class="meta">#</span><span class="bash"> Install basic dependencies</span></span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \</span><br><span class="line">        build-essential \</span><br><span class="line">        cmake \</span><br><span class="line">        git \</span><br><span class="line">        wget \</span><br><span class="line">        libopencv-dev \</span><br><span class="line">        libsnappy-dev \</span><br><span class="line">        python-dev \</span><br><span class="line">        python-pip \</span><br><span class="line">        tzdata \</span><br><span class="line">        vim</span><br><span class="line"><span class="meta">#</span><span class="bash"> Install anaconda <span class="keyword">for</span> python 3.6</span></span><br><span class="line">RUN wget --quiet https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.2.0-Linux-x86_64.sh -O ~/anaconda.sh &amp;&amp; \</span><br><span class="line">    /bin/bash ~/anaconda.sh -b -p /opt/conda &amp;&amp; \</span><br><span class="line">    rm ~/anaconda.sh &amp;&amp; \</span><br><span class="line">    echo &quot;export PATH=/opt/conda/bin:$PATH&quot; &gt;&gt; ~/.bashrc</span><br><span class="line"><span class="meta">#</span><span class="bash"> Set timezone</span></span><br><span class="line">RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br><span class="line"><span class="meta">#</span><span class="bash"> Set locale</span></span><br><span class="line">ENV LANG C.UTF-8 LC_ALL=C.UTF-8</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker build -t tf115:v1 .   #最后运行 docker build -t name:tag</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker cp /home/ubuntu/anaconda3/envs/M para_test1:/opt/conda/envs</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo &quot;. /opt/conda/etc/profile.d/conda.sh&quot; &gt;&gt; ~/.bashrc  </span><br><span class="line">or ln -s /opt/conda/etc/profile.d/conda.sh /etc/profile.d/conda.sh</span><br><span class="line">echo &quot;conda activate M&quot; &gt;&gt; ~/.bashrc </span><br><span class="line">export PATH=&quot;/opt/conda/bin:$PATH&quot;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="nvidia-docker">nvidia-docker</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl status nvidia-docker   # 查看nvidia-docker状态</span><br><span class="line">systemctl start nvidia-docker	 # 启动nvidia-docker服务</span><br><span class="line">systemctl status nvidia-docker	 # 重启后测试 查看状态</span><br></pre></td></tr></table></figure>
<blockquote>
<p>在服务器断电后，NVIDIA服务无法启动的解决办法： 如果出现 error gathering device information while adding custom device 则是因为驱动重装后出现的问题： nvidia-modprobe -u -c=0</p>
<p>如果出现： error looking up volume plugin nvidia-docker plugin nvidia-docker not found 则是因为nvidia-docker服务没启： service nvidia-docker start</p>
</blockquote>
<h3 id="docker内安装conda">docker内安装conda</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 没有wget执行</span></span><br><span class="line">apt-get update</span><br><span class="line">apt-get install wget</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装conda</span></span><br><span class="line">wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.1.0-Linux-x86_64.sh</span><br><span class="line">bash Anaconda3-5.1.0-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>部署</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
</search>
