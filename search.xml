<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>transformer</title>
    <url>/2021/06/19/transformer/</url>
    <content><![CDATA[<h4 id="Transformer模型"><a href="#Transformer模型" class="headerlink" title="Transformer模型"></a>Transformer模型</h4><ul>
<li>论文题目：Attention is all you need</li>
<li>链接：<a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></li>
<li>创新点：<ul>
<li>通过self-attention，自己和自己做attention，使得每个词都有全局的语义信息（长距离依赖）</li>
<li>由于 Self-Attention 是每个词和所有词都要计算 Attention，所以不管他们中间有多长距离，最大的路径长度也都只是 1。可以捕获长距离依赖关系。</li>
<li>提出multi-head attention，可以看成attention的ensemble版本，不同head学习不同的子空间语义。</li>
</ul>
</li>
</ul>
<hr>
<h5 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h5><p>​        以一般的RNN的S2S为例子，一般的attention的Q来自Decoder（如下图中的大H），K和V来自Encoder（如下图中的小h）。self-attention就是attention的K、Q、V都来自encoder或者decoder，使得每个位置的表示都具有全局的语义信息，有利于建立长依赖关系。<img src="https://i.loli.net/2020/11/07/nBfi1yD4MZYc9gK.png" alt="image-20201107163443295"></p>
<blockquote>
<p>self-attention的输入：序列词向量，假设为$X$，$X$经过三个线性变换得到Q，K，V，即$X\times W_Q=Q,X\times W_K=K,X\times W_V=V$，其中$W_Q,W_K,W_V$是不同的，通过训练得到。</p>
<script type="math/tex; mode=display">
Attention(Q,K,V)=Softmax(\frac{QK^T}{\sqrt{d}})V</script><p>优点：</p>
<p>1.可以并行化</p>
<p>2.学习远程依赖</p>
<p>transformer中self-attention的维度dim$=\frac{hidden}{num_head_number}$</p>
</blockquote>
<h5 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h5><p>​        <strong>Batch Normalization</strong>是对一个每一个节点，针对一个batch，做一次normalization，即纵向的normalization:<br><img src="https://i.loli.net/2020/11/07/i3aM8ZlhTpuHDAP.png" alt="image-20201107163849892"><br>layer normalization(LN)，是对一个样本，同一个层网络的所有神经元做normalization，不涉及到batch的概念，即横向normalization:<br><img src="https://i.loli.net/2020/11/07/o8HXFRDrdx4Gwcv.png" alt="image-20201107163933090"><br>​        BN适用于不同mini batch数据分布差异不大的情况，而且BN需要开辟变量存每个节点的均值和方差，空间消耗略大；而且 BN适用于有mini_batch的场景。<br>​        LN只需要一个样本就可以做normalization，可以避免 BN 中受 mini-batch 数据分布影响的问题，也不需要开辟空间存每个节点的均值和方差。<br>​        但是，BN 的转换是针对单个神经元可训练的——不同神经元的输入经过再平移和再缩放后分布在不同的区间，而 LN 对于一整层的神经元训练得到同一个转换——所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小，scale不一样），那么 LN 的处理可能会降低模型的表达能力。</p>
<h4 id="Postional-Encoding"><a href="#Postional-Encoding" class="headerlink" title="Postional Encoding"></a>Postional Encoding</h4><blockquote>
<p>偶数列用sin函数激活</p>
<p>奇数列用cos函数激活</p>
</blockquote>
<h4 id="FFN"><a href="#FFN" class="headerlink" title="FFN"></a>FFN</h4><blockquote>
<p>FFN包含两个线性变换，并且在第一个线性变换后加入ReLU激活函数</p>
<script type="math/tex; mode=display">
FFN (x)=max(0,xW_1+b_1)W_2+b_2</script></blockquote>
<h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><blockquote>
<p>Encoder层：由6个相同的层堆叠而成，每个层包含两个子层，多头注意力层和FFN层。在子层之间加入残差连接和Layer Normalization。</p>
</blockquote>
<h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><blockquote>
<ul>
<li>Encoder与Decoder的不同<ul>
<li>Decoder SubLayer-1 使用的是 “Masked” Multi-Headed Attention 机制，确保在生成时不引入当前词的下文信息，仅依赖于当前词之前的输出信息。这里的mask是一个下三角矩阵，对角线以及对角线左下都是1，其余都是0。</li>
<li>SubLayer-2 是一个 Encoder-Decoder Multi-head Attention。将$h<em>{t}^{dec}$进行线性映射得到$q</em>{t}^{dec}$,将$q_{t}^{dec}$作为查询向量，通过键值对注意力机制来从输入$(K^{enc},V^{enc})$中选取有用的信息。</li>
</ul>
</li>
</ul>
</blockquote>
<h4 id="QA"><a href="#QA" class="headerlink" title="QA"></a>QA</h4><blockquote>
<p><strong>1.Transformer为何使用多头注意力机制？（为什么不使用一个头）</strong></p>
<p>答：多头可以使参数矩阵形成多个子空间，矩阵整体的size不变，只是改变了每个head对应的维度大小，这样做使矩阵对多方面信息进行学习，但是计算量和单个head差不多。</p>
<p><strong>2.Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？</strong></p>
<p>答：请求和键值初始为不同的权重是为了解决可能输入句长与输出句长不一致的问题。并且假如QK维度一致，如果不用Q，直接拿K和K点乘的话,即$KK^T$，结果为一个对称矩阵，你会发现attention score 矩阵是一个对称矩阵。因为是同样一个矩阵，都投影到了同样一个空间，所以<strong>泛化能力很差</strong>。</p>
<p><strong>3.Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？</strong></p>
<p>答：K和Q的点乘是为了得到一个attention score 矩阵，用来<strong>对V进行提纯</strong>。K和Q使用了不同的$W_k, W_Q$来计算，可以理解为是在不同空间上的投影。正因为 有了这种不同空间的投影，增加了表达能力，这样计算得到的attention score矩阵的泛化能力更高。</p>
<p><strong>4.为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解</strong></p>
<p>答：假设 Q 和 K 的均值为0，方差为1。它们的矩阵乘积$q\cdot k=\sum_{i=1}^{d_k}q_ik_i$的均值为0，方差为$d_k$，因此使用$d_k$的平方根被用于缩放，因为，Q 和 K 的矩阵乘积的均值本应该为 0，方差本应该为1，这样可以获得更平缓的softmax。当维度很大时，点积结果会很大，会导致softmax的梯度很小。为了减轻这个影响，对点积进行缩放。<a href="https://www.zhihu.com/question/339723385/answer/782509914">知乎解答</a></p>
<p><strong>5.在计算attention score的时候如何对padding做mask操作？</strong></p>
<p>答：对需要mask的位置设为负无穷，再对attention score进行相加</p>
<p><strong>6.为什么在进行多头注意力的时候需要对每个head进行降维？</strong></p>
<p>答：将原有的高维空间转化为多个<strong>低维</strong>空间并再最后进行拼接，形成同样维度的输出，借此<strong>丰富特征信息</strong>，<strong>降低了计算量</strong></p>
<p><strong>8.为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？</strong></p>
<p>embedding matrix的初始化方式是xavier init，这种方式的方差是1/embedding size，因此乘以embedding size的开方使得embedding matrix的方差是1，在这个scale下可能更有利于embedding matrix的收敛。</p>
<p><strong>9.简单介绍一下Transformer的位置编码？有什么意义和优缺点？</strong></p>
<p>答：因为self-attention是位置无关的，无论句子的顺序是什么样的，通过self-attention计算的token的hidden embedding都是一样的，这显然不符合人类的思维。因此要有一个办法能够在模型中表达出一个token的位置信息，transformer使用了固定的positional encoding来表示token在句子中的绝对位置信息。</p>
<p><strong>10.你还了解哪些关于位置编码的技术，各自的优缺点是什么？</strong></p>
<p>答：相对位置编码（RPE）1.在计算attention score和weighted value时各加入一个可训练的表示相对位置的参数。2.在生成多头注意力时，把对key来说将绝对位置转换为相对query的位置3.复数域函数，已知一个词在某个位置的词向量表示，可以计算出它在任何位置的词向量表示。前两个方法是词向量+位置编码，属于亡羊补牢，复数域是生成词向量的时候即生成对应的位置信息。</p>
<p><strong>11.简单讲一下Transformer中的残差结构以及意义。</strong></p>
<p>答：encoder和decoder的self-attention层和ffn层都有残差连接。反向传播的时候不会造成梯度消失。</p>
<p><strong>12.为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？</strong></p>
<p>答：多头注意力层和激活函数层之间。CV使用BN是认为channel维度的信息对cv方面有重要意义，如果对channel维度也归一化会造成不同通道信息一定的损失。而同理nlp领域认为句子长度不一致，并且各个batch的信息没什么关系，因此只考虑句子内信息的归一化，也就是LN。</p>
<p><strong>13.简答讲一下BatchNorm技术，以及它的优缺点。</strong></p>
<p>答：批归一化是对每一批的数据在进入激活函数前进行归一化，可以<strong>提高收敛速度</strong>，<strong>防止过拟合</strong>，<strong>防止梯度消失</strong>，增加网络对数据的敏感度。</p>
<p><strong>14.简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？</strong></p>
<p>答：输入嵌入-加上位置编码-多个编码器层（每个编码器层包含全连接层，多头注意力层和点式前馈网络层（包含激活函数层））-多个解码器层（每个编码器层包含全连接层，多头注意力层和点式前馈网络层）-全连接层，使用了relu激活函数</p>
<p><strong>15.Encoder端和Decoder端是如何进行交互的？</strong></p>
<p>答：通过转置encoder_ouput的seq_len维与depth维，进行矩阵两次乘法，即q<em>kT</em>v输出即可得到target_len维度的输出</p>
<p><strong>16.Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？</strong></p>
<p>答：Decoder有<strong>两层</strong>mha，encoder有<strong>一层</strong>mha，Decoder的第二层mha是为了转化输入与输出句长，Decoder的请求q与键k和数值v的倒数第二个维度可以不一样，但是encoder的qkv维度一样。</p>
<p><strong>17.Transformer的并行化提现在哪个地方？</strong></p>
<p>答：Transformer的并行化主要体现在self-attention模块，在Encoder端Transformer可以并行处理整个序列，并得到整个输入序列经过Encoder端的输出，但是rnn只能从前到后的执行</p>
<p><strong>17.Transformer的并行化提现在哪个地方？</strong></p>
<p>答：Transformer的并行化主要体现在self-attention模块，在Encoder端Transformer可以并行处理整个序列，并得到整个输入序列经过Encoder端的输出，但是rnn只能从前到后的执行</p>
<p><strong>19.简单描述一下wordpiece model 和 byte pair encoding，有实际应用过吗？</strong></p>
<p>答“传统词表示方法无法很好的处理未知或罕见的词汇（<strong>OOV问题</strong>）<br>传统词tokenization方法不利于模型学习词缀之间的关系”</p>
<p>BPE（字节对编码）或二元编码是一种简单的数据压缩形式，其中最常见的一对连续字节数据被替换为该数据中不存在的字节。后期使用时需要一个替换表来重建原始数据。</p>
<p>优点：可以有效地平衡词汇表大小和步数（编码句子所需的token次数）。<br>缺点：基于贪婪和确定的符号替换，不能提供带概率的多个分片结果。</p>
<p><strong>20.Transformer训练的时候学习率是如何设定的？Dropout是如何设定的，位置在哪里？Dropout 在测试的需要有什么需要注意的吗？</strong></p>
<p>LN是为了解决梯度消失的问题，dropout是为了解决过拟合的问题。在embedding后面加LN有利于embedding matrix的收敛。</p>
<p><strong>21.bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？</strong></p>
<p>答：BERT和transformer的<strong>目标不一致</strong>，bert是语言的预训练模型，需要充分考虑上下文的关系，而transformer主要考虑句子中第i个元素与前i-1个元素的关系。</p>
</blockquote>
<h4 id="Refenrence"><a href="#Refenrence" class="headerlink" title="Refenrence"></a>Refenrence</h4><blockquote>
<p><a href="https://blog.csdn.net/lty_sky/article/details/105321180?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3.not_use_machine_learn_pai&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3.not_use_machine_learn_pai">代码和原理</a></p>
<p><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">Harvard版本代码和原理</a></p>
</blockquote>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>transformer</tag>
      </tags>
  </entry>
</search>
